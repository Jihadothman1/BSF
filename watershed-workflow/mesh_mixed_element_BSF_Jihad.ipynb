{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Mixed-Element Mesh for Delineated Watershed\n",
    "\n",
    "This workflow provides a complete working example to develop an streamaligned mixed-element mesh for Coweeta watershed. Long quad elements with pentagons at junctions are placed along NHDPlus flowlines to represent rivers/streams. Rest of the domain is meshed with standard TIN.\n",
    "\n",
    "It uses the following datasets:\n",
    "\n",
    "* `NHD Plus` for the watershed boundary and hydrography.\n",
    "* `NED` for elevation\n",
    "* `NLCD` for land cover/transpiration/rooting depths\n",
    "* `GLYHMPS` geology data for structural formations\n",
    "* `SoilGrids 2017` for depth to bedrock and soil texture information\n",
    "* `SSURGO` for soil data, where available, in the top 2m.\n",
    "\n",
    "This workflow creates the following files:\n",
    "\n",
    "* Mesh file: `Coweeta.exo`, includes all labeled sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing all the package needed to run teh watershed workflow\n",
    "# conda package imports\n",
    "import os,sys\n",
    "import numpy as np\n",
    "import pandas\n",
    "from matplotlib import pyplot as plt\n",
    "import logging\n",
    "import pandas as pd\n",
    "import copy\n",
    "import scipy\n",
    "import shapely\n",
    "\n",
    "import watershed_workflow \n",
    "import watershed_workflow.source_list\n",
    "import watershed_workflow.ui\n",
    "import watershed_workflow.utils\n",
    "import watershed_workflow.plot\n",
    "import watershed_workflow.mesh\n",
    "import watershed_workflow.densification\n",
    "import watershed_workflow.condition\n",
    "import watershed_workflow.regions\n",
    "watershed_workflow.ui.setup_logging(1,None)\n",
    "\n",
    "#if there is an error, some packages need to be upgraded. Thus, comment out this cell and uncomment the one below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# import os,sys\n",
    "# import numpy as np\n",
    "# import pandas\n",
    "# from matplotlib import pyplot as plt\n",
    "# import logging\n",
    "# import pandas as pd\n",
    "# import copy\n",
    "# import scipy\n",
    "# import shapely\n",
    "# import watershed_workflow \n",
    "# import watershed_workflow.source_list\n",
    "# import watershed_workflow.ui\n",
    "# import watershed_workflow.utils\n",
    "# import watershed_workflow.plot\n",
    "# import watershed_workflow.mesh\n",
    "\n",
    "\n",
    "# !pip install --upgrade rasterio\n",
    "# !pip install --upgrade fiona\n",
    "# !pip install --upgrade netCDF4\n",
    "# !pip install --upgrade h5py\n",
    "\n",
    "\n",
    "# import watershed_workflow.densification\n",
    "# import watershed_workflow.condition\n",
    "# import watershed_workflow.regions\n",
    "# watershed_workflow.ui.setup_logging(1,None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Importing all the package needed to run teh watershed workflow\n",
    "# conda package imports\n",
    "import os,sys\n",
    "import numpy as np\n",
    "import pandas\n",
    "from matplotlib import pyplot as plt\n",
    "import logging\n",
    "import pandas as pd\n",
    "import copy\n",
    "import scipy\n",
    "import shapely\n",
    "\n",
    "import watershed_workflow \n",
    "import watershed_workflow.source_list\n",
    "import watershed_workflow.ui\n",
    "import watershed_workflow.utils\n",
    "import watershed_workflow.plot\n",
    "import watershed_workflow.mesh\n",
    "import watershed_workflow.densification\n",
    "import watershed_workflow.condition\n",
    "import watershed_workflow.regions\n",
    "watershed_workflow.ui.setup_logging(1,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameters cell -- this provides all parameters that can be changed via pipelining to generate a new watershed. \n",
    "BSF_shapefile = './BSF/input_data/BSF_USGS_UTM_12.shp'\n",
    "hint = '1601'  # hint: HUC 4 containing this shape.  \n",
    "               # This is necessary to avoid downloading all HUCs to search for this shape\n",
    "name = 'BSF'\n",
    "modis_name = None\n",
    "\n",
    "figsize = (6,6)\n",
    "figsize_3d = (8,6)\n",
    "\n",
    "# Geomtric parameters tuning the degree of cleaning of the raw data and scales of hydrologic features to be considered\n",
    "simplify = 60 # length scale to target average edge\n",
    "ignore_small_rivers = 2 \n",
    "prune_by_area_fraction = 0.01 \n",
    "\n",
    "# huc boundary refinement control\n",
    "refine_d0 = 20\n",
    "refine_d1 = 100\n",
    "refine_L0 = 70\n",
    "refine_L1 = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that, by default, we tend to work in the DayMet CRS because this allows us to avoid\n",
    "# reprojecting meteorological forcing datasets.\n",
    "crs = watershed_workflow.crs.daymet_crs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources and setup\n",
    "\n",
    "Next we set up the source watershed and coordinate system and all data sources for our mesh.  We will use the CRS that is included in the shapefile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A wide range of data sources are available; here we use the defaults except for using NHD Plus for watershed boundaries and hydrography (the default is NHD, which is lower resolution and therefore smaller download sizes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-15 11:56:05,291 - root - INFO: Using sources:\n",
      "2023-11-15 11:56:05,293 - root - INFO: --------------\n",
      "2023-11-15 11:56:05,295 - root - INFO: HUC: National Hydrography Dataset Plus High Resolution (NHDPlus HR)\n",
      "2023-11-15 11:56:05,295 - root - INFO: hydrography: National Hydrography Dataset Plus High Resolution (NHDPlus HR)\n",
      "2023-11-15 11:56:05,295 - root - INFO: DEM: National Elevation Dataset (NED)\n",
      "2023-11-15 11:56:05,295 - root - INFO: soil structure: National Resources Conservation Service Soil Survey (NRCS Soils)\n",
      "2023-11-15 11:56:05,295 - root - INFO: geologic structure: BSF/input_data/GLHYMPS/GLHYMPS_BSF.shp\n",
      "2023-11-15 11:56:05,295 - root - INFO: land cover: raster\n",
      "2023-11-15 11:56:05,295 - root - INFO: lai: MODIS\n",
      "2023-11-15 11:56:05,295 - root - INFO: soil thickness: None\n",
      "2023-11-15 11:56:05,295 - root - INFO: meteorology: DayMet 1km\n",
      "2023-11-15 11:56:05,295 - root - INFO: depth to bedrock: raster\n"
     ]
    }
   ],
   "source": [
    "# set up a dictionary of source objects\n",
    "sources = watershed_workflow.source_list.get_default_sources()\n",
    "sources['hydrography'] = watershed_workflow.source_list.hydrography_sources['NHD Plus']\n",
    "sources['HUC'] = watershed_workflow.source_list.huc_sources['NHD Plus']\n",
    "sources['depth to bedrock'] = watershed_workflow.source_list.FileManagerSoilGrids2017()\n",
    "\n",
    "\n",
    "#\n",
    "# This demo uses a few datasets that have been clipped out of larger, national\n",
    "# datasets and are distributed with the code.  This is simply to save download\n",
    "# time for this simple problem and to lower the barrier for trying out\n",
    "# Watershed Workflow.  A more typical workflow would delete these lines (as \n",
    "# these files would not exist for other watersheds).\n",
    "#\n",
    "# The default versions of these download large raster and shapefile files that\n",
    "# are defined over a very large region (globally or the entire US).\n",
    "#\n",
    "# Note we also prepopulate some data for MODIS data as well.\n",
    "#\n",
    "sources['land cover'] = watershed_workflow.source_list.FileManagerRaster('BSF/input_data/land_cover/LC_BSF.tif')\n",
    "sources['geologic structure'] = watershed_workflow.source_list.FileManagerGLHYMPS('BSF/input_data/GLHYMPS/GLHYMPS_BSF.shp')\n",
    "sources['depth to bedrock'] = watershed_workflow.source_list.FileManagerRaster('BSF/input_data/DTB/DTB.tif')\n",
    "watershed_workflow.source_list.log_sources(sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get HUCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-15 11:56:05,314 - root - INFO: \n",
      "2023-11-15 11:56:05,314 - root - INFO: Loading shapes\n",
      "2023-11-15 11:56:05,314 - root - INFO: ------------------------------\n",
      "2023-11-15 11:56:05,315 - root - INFO: Loading file: './BSF/input_data/BSF_USGS_UTM_12.shp'\n",
      "2023-11-15 11:56:05,383 - root - INFO: ... found 1 shapes\n",
      "2023-11-15 11:56:05,384 - root - INFO: Converting to shapely\n",
      "2023-11-15 11:56:05,386 - root - INFO:  ... done\n",
      "2023-11-15 11:56:05,393 - root - INFO: Converting to requested CRS\n",
      "2023-11-15 11:56:05,437 - root - INFO:  ... done\n",
      "2023-11-15 11:56:05,437 - root - INFO: Removing holes on 1 polygons\n",
      "2023-11-15 11:56:05,452 - root - INFO:   -- removed interior\n",
      "2023-11-15 11:56:05,453 - root - INFO:   -- union\n",
      "2023-11-15 11:56:05,454 - root - INFO: Parsing 1 components for holes\n",
      "2023-11-15 11:56:05,454 - root - INFO:   -- complete\n"
     ]
    }
   ],
   "source": [
    "# load hucs from shape\n",
    "_, watershed = watershed_workflow.get_split_form_shapes(BSF_shapefile, out_crs=crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Rivers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-15 11:56:05,460 - root - INFO: \n",
      "2023-11-15 11:56:05,468 - root - INFO: Loading Hydrography\n",
      "2023-11-15 11:56:05,469 - root - INFO: ------------------------------\n",
      "2023-11-15 11:56:05,470 - root - INFO: Loading streams in HUC 1601\n",
      "2023-11-15 11:56:05,471 - root - INFO:          and/or bounds (-938147.4704816904, -46858.27379953274, -903075.9019984158, -6172.107564101888)\n",
      "2023-11-15 11:56:05,473 - root - INFO:   Using Hydrography file \"C:\\Users\\jihad\\watershed-workflow\\examples\\data\\hydrography\\NHDPlus_H_1601_GDB\\NHDPlus_H_1601.gdb\"\n",
      "2023-11-15 11:56:05,473 - root - INFO:   National Hydrography Dataset Plus High Resolution (NHDPlus HR): opening 'C:\\Users\\jihad\\watershed-workflow\\examples\\data\\hydrography\\NHDPlus_H_1601_GDB\\NHDPlus_H_1601.gdb' layer 'NHDFlowline' for streams in '(-938147.4704816904, -46858.27379953274, -903075.9019984158, -6172.107564101888)'\n",
      "2023-11-15 11:56:06,301 - root - INFO:   Found total of 3233 in bounds.\n",
      "2023-11-15 11:56:06,301 - root - INFO:   Filtering reaches not in-network\n",
      "2023-11-15 11:56:09,991 - root - INFO: ... found 3220 reaches\n",
      "2023-11-15 11:56:09,991 - root - INFO: Converting to shapely\n",
      "2023-11-15 11:56:10,275 - root - INFO:  ... done\n",
      "2023-11-15 11:56:10,276 - root - INFO: Converting to out_crs\n",
      "2023-11-15 11:56:10,277 - root - INFO:   EPSG:4269\n",
      "2023-11-15 11:56:10,278 - root - INFO:   +proj=lcc +lat_1=25 +lat_2=60 +lat_0=42.5 +lon_0=-100 +x_0=0 +y_0=0 +ellps=WGS84 +units=m +no_defs +type=crs\n",
      "2023-11-15 11:56:10,429 - root - INFO: (-944031.5791292262, -52110.54764035802, -896769.4768673662, -984.3429197787692)\n",
      "2023-11-15 11:56:10,430 - root - INFO:  ... done\n",
      "2023-11-15 11:56:11,356 - root - INFO: Removed 2161 of 3220 reaches not in shape\n",
      "2023-11-15 11:56:11,358 - root - INFO: \n",
      "2023-11-15 11:56:11,358 - root - INFO: Constructing river network\n",
      "2023-11-15 11:56:11,359 - root - INFO: ------------------------------\n",
      "2023-11-15 11:56:11,359 - root - INFO: Generating the river tree\n",
      "2023-11-15 11:56:11,371 - root - INFO:  ... generated 17 rivers\n",
      "2023-11-15 11:56:11,372 - root - INFO: Removing rivers with fewer than 2 reaches.\n",
      "2023-11-15 11:56:11,376 - root - INFO: ... removed 12 rivers\n",
      "2023-11-15 11:56:11,376 - root - INFO: Removing rivers with area < 6.763452382969608\n",
      "2023-11-15 11:56:11,377 - root - INFO: Removing divergent sections...\n",
      "2023-11-15 11:56:11,391 - root - INFO:   ... removed 10 divergence tributaries with 15 total reaches.\n",
      "2023-11-15 11:56:11,392 - root - INFO:   ... removed 1 divergence tributaries with 1 total reaches.\n",
      "2023-11-15 11:56:11,393 - root - INFO: Pruning by total contributing area < 6.763452382969608\n",
      "2023-11-15 11:56:11,430 - root - INFO: ... pruned 691\n",
      "2023-11-15 11:56:11,430 - root - INFO: Removing rivers with fewer than 2 reaches.\n",
      "2023-11-15 11:56:11,430 - root - INFO: ... removed 0 rivers\n",
      "2023-11-15 11:56:11,474 - root - INFO: \n",
      "2023-11-15 11:56:11,474 - root - INFO: Simplifying\n",
      "2023-11-15 11:56:11,474 - root - INFO: ------------------------------\n",
      "2023-11-15 11:56:11,474 - root - INFO: Simplifying rivers\n",
      "2023-11-15 11:56:11,523 - root - INFO:   ...cleaned inner segment of length 42.2365 at centroid (-931215.6016107664, -27416.806378894194)\n",
      "2023-11-15 11:56:11,525 - root - INFO:   ...cleaned inner segment of length 19.9946 at centroid (-928677.7126336083, -26900.63062462926)\n",
      "2023-11-15 11:56:11,529 - root - INFO:   ...cleaned inner segment of length 14.5143 at centroid (-925290.0096967305, -27606.730775906683)\n",
      "2023-11-15 11:56:11,531 - root - INFO:   ...cleaned inner segment of length 35.8552 at centroid (-917460.226348375, -40848.93074384969)\n",
      "2023-11-15 11:56:11,531 - root - INFO:   ...cleaned inner segment of length 37.2435 at centroid (-917437.7513943594, -40877.325113829706)\n",
      "2023-11-15 11:56:11,532 - root - INFO:   ...cleaned inner segment of length 15.2301 at centroid (-917378.0169243353, -40967.00149628192)\n",
      "2023-11-15 11:56:11,533 - root - INFO:   ...cleaned inner segment of length 34.8693 at centroid (-917367.6589495341, -40989.594573901355)\n",
      "2023-11-15 11:56:11,534 - root - INFO:   ...cleaned inner segment of length 16.9931 at centroid (-917357.4613736048, -41013.33841584841)\n",
      "2023-11-15 11:56:11,535 - root - INFO:   ...cleaned inner segment of length 30.8369 at centroid (-917336.7682747304, -41136.640887438094)\n",
      "2023-11-15 11:56:11,537 - root - INFO:   ...cleaned inner segment of length 48.1291 at centroid (-917324.8532688353, -41171.196672799786)\n",
      "2023-11-15 11:56:11,538 - root - INFO:   ...cleaned inner segment of length 55.9531 at centroid (-917293.9855359781, -41213.094580169534)\n",
      "2023-11-15 11:56:11,539 - root - INFO:   ...cleaned inner segment of length 5.71978 at centroid (-916976.7870677301, -41777.02624309322)\n",
      "2023-11-15 11:56:11,540 - root - INFO:   ...cleaned inner segment of length 53.4422 at centroid (-916993.4542609996, -41799.66392153809)\n",
      "2023-11-15 11:56:11,540 - root - INFO:   ...cleaned inner segment of length 19.365 at centroid (-917004.8777864067, -41826.99039493059)\n",
      "2023-11-15 11:56:11,540 - root - INFO:   ...cleaned inner segment of length 9.32221 at centroid (-915953.0316012132, -42823.33195898947)\n",
      "2023-11-15 11:56:11,540 - root - INFO:   ...cleaned inner segment of length 6.02797 at centroid (-915946.1713813154, -42825.72977224519)\n",
      "2023-11-15 11:56:11,540 - root - INFO:   ...cleaned inner segment of length 12.2746 at centroid (-915944.6615979085, -42833.873593753415)\n",
      "2023-11-15 11:56:11,540 - root - INFO:   ...cleaned inner segment of length 38.1123 at centroid (-915930.5932053949, -42852.01030853327)\n",
      "2023-11-15 11:56:11,540 - root - INFO:   ...cleaned inner segment of length 20.0327 at centroid (-915907.499224401, -42869.6227546188)\n",
      "2023-11-15 11:56:11,540 - root - INFO:   ...cleaned inner segment of length 46.5857 at centroid (-917888.411294342, -40423.55605869019)\n",
      "2023-11-15 11:56:11,540 - root - INFO:   ...cleaned inner segment of length 57.7356 at centroid (-914306.8176273075, -35862.97623432862)\n",
      "2023-11-15 11:56:11,540 - root - INFO:   ...cleaned inner segment of length 17.5451 at centroid (-912707.9712966466, -39799.34364336732)\n",
      "2023-11-15 11:56:11,540 - root - INFO:   ...cleaned inner segment of length 7.77556 at centroid (-912731.0590920261, -40821.37177791858)\n",
      "2023-11-15 11:56:11,540 - root - INFO:   ...cleaned inner segment of length 54.6603 at centroid (-917227.5176949375, -34324.027838440496)\n",
      "2023-11-15 11:56:11,553 - root - INFO:   ...cleaned inner segment of length 21.1842 at centroid (-919750.162900551, -37209.36027757719)\n",
      "2023-11-15 11:56:11,554 - root - INFO:   ...cleaned inner segment of length 32.4395 at centroid (-921212.6181830876, -40831.167667950125)\n",
      "2023-11-15 11:56:11,555 - root - INFO:   ...cleaned inner segment of length 38.8792 at centroid (-917997.0203558272, -36128.53973110493)\n",
      "2023-11-15 11:56:11,557 - root - INFO:   ...cleaned inner segment of length 16.908 at centroid (-909501.2415365954, -30172.718229570695)\n",
      "2023-11-15 11:56:11,557 - root - INFO:   ...cleaned inner segment of length 3.49776 at centroid (-909491.2334679614, -30174.685149899098)\n",
      "2023-11-15 11:56:11,559 - root - INFO:   ...cleaned inner segment of length 8.73279 at centroid (-909485.1452904344, -30175.228678328986)\n",
      "2023-11-15 11:56:11,560 - root - INFO:   ...cleaned inner segment of length 26.3867 at centroid (-909467.672472412, -30174.114926660164)\n",
      "2023-11-15 11:56:11,561 - root - INFO:   ...cleaned inner segment of length 21.932 at centroid (-909443.912148032, -30170.056965062846)\n",
      "2023-11-15 11:56:11,561 - root - INFO:   ...cleaned inner segment of length 33.8484 at centroid (-909417.4617736385, -30161.36941153588)\n",
      "2023-11-15 11:56:11,563 - root - INFO:   ...cleaned inner segment of length 31.605 at centroid (-912254.265969479, -32561.384856877972)\n",
      "2023-11-15 11:56:11,564 - root - INFO:   ...cleaned inner segment of length 59.9571 at centroid (-913895.571746068, -27595.00034431783)\n",
      "2023-11-15 11:56:11,565 - root - INFO:   ...cleaned inner segment of length 42.5618 at centroid (-913853.8758959122, -27616.085723806005)\n",
      "2023-11-15 11:56:11,566 - root - INFO:   ...cleaned inner segment of length 43.7451 at centroid (-912605.2226116387, -27441.669986133125)\n",
      "2023-11-15 11:56:11,567 - root - INFO:   ...cleaned inner segment of length 58.3743 at centroid (-912558.8135644108, -27420.538519703994)\n",
      "2023-11-15 11:56:11,568 - root - INFO:   ...cleaned inner segment of length 11.8674 at centroid (-912527.4819001456, -27413.75498156466)\n",
      "2023-11-15 11:56:11,569 - root - INFO:   ...cleaned inner segment of length 27.8854 at centroid (-912510.7056631645, -27424.030367104824)\n",
      "2023-11-15 11:56:11,570 - root - INFO:   ...cleaned inner segment of length 8.32788 at centroid (-908412.0830734319, -27065.786348809823)\n",
      "2023-11-15 11:56:11,571 - root - INFO:   ...cleaned inner segment of length 45.2608 at centroid (-905662.513921526, -26649.46361322835)\n",
      "2023-11-15 11:56:11,572 - root - INFO:   ...cleaned inner segment of length 6.64936 at centroid (-905641.6073901921, -26634.145467877363)\n",
      "2023-11-15 11:56:11,573 - root - INFO:   ...cleaned inner segment of length 16.0882 at centroid (-905631.6012930007, -26629.26232826451)\n",
      "2023-11-15 11:56:11,574 - root - INFO:   ...cleaned inner segment of length 32.7332 at centroid (-905607.8693021407, -26629.602269095143)\n",
      "2023-11-15 11:56:11,574 - root - INFO:   ...cleaned inner segment of length 16.0483 at centroid (-905586.7508136816, -26638.77192153632)\n",
      "2023-11-15 11:56:11,576 - root - INFO:   ...cleaned inner segment of length 54.7685 at centroid (-905436.211287699, -26654.26401777361)\n",
      "2023-11-15 11:56:11,576 - root - INFO:   ...cleaned inner segment of length 33.8273 at centroid (-923172.7673885264, -34481.39114226598)\n",
      "2023-11-15 11:56:11,577 - root - INFO:   ...cleaned inner segment of length 38.926 at centroid (-923951.3716567818, -30322.775248883292)\n",
      "2023-11-15 11:56:11,579 - root - INFO:   ...cleaned inner segment of length 13.5624 at centroid (-917341.1737590397, -23567.77317371191)\n",
      "2023-11-15 11:56:11,580 - root - INFO:   ...cleaned inner segment of length 51.8028 at centroid (-907207.2626518287, -15005.183112978279)\n",
      "2023-11-15 11:56:11,581 - root - INFO:   ...cleaned inner segment of length 18.5791 at centroid (-907187.4131741915, -14976.220329878757)\n",
      "2023-11-15 11:56:11,582 - root - INFO:   ...cleaned inner segment of length 36.2366 at centroid (-907250.3251865692, -14651.337407520068)\n",
      "2023-11-15 11:56:11,582 - root - INFO:   ...cleaned inner segment of length 18.5574 at centroid (-907254.1064037845, -14548.943809257984)\n",
      "2023-11-15 11:56:11,583 - root - INFO:   ...cleaned inner segment of length 43.4313 at centroid (-907266.0476746964, -14520.58142328178)\n",
      "2023-11-15 11:56:11,584 - root - INFO:   ...cleaned inner segment of length 10.9721 at centroid (-906864.2486841449, -13249.29491706591)\n",
      "2023-11-15 11:56:11,585 - root - INFO:   ...cleaned inner segment of length 22.374 at centroid (-907012.4305986763, -22539.779258026523)\n",
      "2023-11-15 11:56:11,585 - root - INFO:   ...cleaned inner segment of length 23.2943 at centroid (-907013.2271158183, -22562.567085897066)\n",
      "2023-11-15 11:56:11,585 - root - INFO:   ...cleaned inner segment of length 34.2457 at centroid (-907013.6648967213, -22591.2831813957)\n",
      "2023-11-15 11:56:11,589 - root - INFO:   ...cleaned inner segment of length 52.8104 at centroid (-906830.6885422892, -20531.08613932413)\n",
      "2023-11-15 11:56:11,589 - root - INFO:   ...cleaned inner segment of length 16.5444 at centroid (-923262.388763778, -22700.985249007805)\n",
      "2023-11-15 11:56:11,589 - root - INFO: Simplifying HUCs\n",
      "2023-11-15 11:56:11,604 - root - INFO: Snapping river and HUC (nearly) coincident nodes\n",
      "2023-11-15 11:56:11,611 - root - INFO:   snapping polygon segment boundaries to river endpoints\n",
      "2023-11-15 11:56:11,622 - root - INFO:   snapping river endpoints to the polygon\n",
      "2023-11-15 11:56:11,625 - root - INFO:     snapped river: (-937768.4632815605, -17445.1253789166) to (-937775.6219109343, -17498.93180745932)\n",
      "2023-11-15 11:56:11,628 - root - INFO:   - snapped river: (-937768.4632815605, -17445.1253789166) to (-937775.6219109343, -17498.93180745932)\n",
      "2023-11-15 11:56:11,630 - root - INFO:     snapped river: (-937664.2028711599, -17595.718918452458) to (-937653.4877788831, -17515.181036574315)\n",
      "2023-11-15 11:56:11,632 - root - INFO:   - snapped river: (-937768.4632815605, -17445.1253789166) to (-937775.6219109343, -17498.93180745932)\n",
      "2023-11-15 11:56:11,634 - root - INFO:   - snapped river: (-937664.2028711599, -17595.718918452458) to (-937653.4877788831, -17515.181036574315)\n",
      "2023-11-15 11:56:11,935 - root - INFO:   cutting at crossings\n",
      "2023-11-15 11:56:11,936 - root - INFO: intersection found\n",
      "2023-11-15 11:56:11,936 - root - INFO:   - cutting reach at external boundary of HUCs:\n",
      "2023-11-15 11:56:11,936 - root - INFO:       split HUC boundary seg into 1 pieces\n",
      "2023-11-15 11:56:11,936 - root - INFO:       split reach seg into 1 pieces\n",
      "2023-11-15 11:56:11,936 - root - INFO: intersection found\n",
      "2023-11-15 11:56:11,936 - root - INFO:   - cutting reach at external boundary of HUCs:\n",
      "2023-11-15 11:56:11,936 - root - INFO:       split HUC boundary seg into 1 pieces\n",
      "2023-11-15 11:56:11,936 - root - INFO:       split reach seg into 1 pieces\n",
      "2023-11-15 11:56:11,936 - root - INFO: intersection found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dual/multiple section: type = <class 'shapely.geometry.linestring.LineString'>\n",
      " point = LINESTRING (-937775.6219109343 -17498.93180745932, -937653.4877788831 -17515.181036574315)\n"
     ]
    },
    {
     "ename": "CutError",
     "evalue": "Dual/multiple intersection in a single seg... ugh!  Intersection is of type '<class 'shapely.geometry.linestring.LineString'>'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCutError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m watershed_orig\u001b[38;5;241m=\u001b[39mcopy\u001b[38;5;241m.\u001b[39mdeepcopy(watershed) \n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# simplifying \u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m rivers \u001b[38;5;241m=\u001b[39m \u001b[43mwatershed_workflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimplify\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwatershed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrivers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msimplify_hucs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msimplify\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msimplify_rivers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msimplify\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m                           \u001b[49m\u001b[43msnap_tol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.75\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msimplify\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcut_intersections\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# for plotting purpose only\u001b[39;00m\n\u001b[0;32m     24\u001b[0m rivers_simplified\u001b[38;5;241m=\u001b[39m[river\u001b[38;5;241m.\u001b[39mdeepcopy() \u001b[38;5;28;01mfor\u001b[39;00m river \u001b[38;5;129;01min\u001b[39;00m rivers] \n",
      "File \u001b[1;32m~\\watershed-workflow\\watershed_workflow\\__init__.py:1059\u001b[0m, in \u001b[0;36msimplify\u001b[1;34m(hucs, rivers, waterbodies, simplify_hucs, simplify_rivers, simplify_waterbodies, prune_tol, merge_tol, snap_tol, snap_triple_junctions_tol, snap_reach_endpoints_tol, snap_waterbodies_tol, cut_intersections)\u001b[0m\n\u001b[0;32m   1057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m snap_tol \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m snap_triple_junctions_tol \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m snap_reach_endpoints_tol \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m cut_intersections:\n\u001b[0;32m   1058\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSnapping river and HUC (nearly) coincident nodes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1059\u001b[0m     rivers \u001b[38;5;241m=\u001b[39m \u001b[43mwatershed_workflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhydrography\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msnap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhucs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrivers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msnap_tol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1060\u001b[0m \u001b[43m                                                 \u001b[49m\u001b[43msnap_triple_junctions_tol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1061\u001b[0m \u001b[43m                                                 \u001b[49m\u001b[43msnap_reach_endpoints_tol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcut_intersections\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1063\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m simplify_waterbodies \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m waterbodies \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1064\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, wb \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(waterbodies):\n",
      "File \u001b[1;32m~\\watershed-workflow\\watershed_workflow\\hydrography.py:114\u001b[0m, in \u001b[0;36msnap\u001b[1;34m(hucs, rivers, tol, triple_junctions_tol, reach_endpoints_tol, cut_intersections)\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cut_intersections:\n\u001b[1;32m--> 114\u001b[0m     \u001b[43mcut_and_snap_crossings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhucs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrivers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;66;03m# snapping can result in 0-length reaches\u001b[39;00m\n\u001b[0;32m    117\u001b[0m cleanup(rivers)\n",
      "File \u001b[1;32m~\\watershed-workflow\\watershed_workflow\\hydrography.py:162\u001b[0m, in \u001b[0;36mcut_and_snap_crossings\u001b[1;34m(hucs, rivers, tol)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tree \u001b[38;5;129;01min\u001b[39;00m rivers:\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m river_node \u001b[38;5;129;01min\u001b[39;00m tree\u001b[38;5;241m.\u001b[39mpreOrder():\n\u001b[1;32m--> 162\u001b[0m         \u001b[43m_cut_and_snap_crossing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhucs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mriver_node\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m cleanup(rivers)\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m rivers\n",
      "File \u001b[1;32m~\\watershed-workflow\\watershed_workflow\\hydrography.py:181\u001b[0m, in \u001b[0;36m_cut_and_snap_crossing\u001b[1;34m(hucs, reach_node, tol)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m seg\u001b[38;5;241m.\u001b[39mintersects(r):\n\u001b[0;32m    180\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mintersection found\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 181\u001b[0m     new_spine \u001b[38;5;241m=\u001b[39m \u001b[43mwatershed_workflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcut\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m     new_reach_segs \u001b[38;5;241m=\u001b[39m watershed_workflow\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcut(r, seg, tol)\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\watershed-workflow\\watershed_workflow\\utils.py:682\u001b[0m, in \u001b[0;36mcut\u001b[1;34m(line, cutline, tol)\u001b[0m\n\u001b[0;32m    680\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDual/multiple section: type = \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m(point)))\n\u001b[0;32m    681\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m point = \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(point))\n\u001b[1;32m--> 682\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m CutError(\n\u001b[0;32m    683\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDual/multiple intersection in a single seg... ugh!  \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    684\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIntersection is of type \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m(point)), line, seg, cutline)\n\u001b[0;32m    686\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(segcoords) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    687\u001b[0m     segs\u001b[38;5;241m.\u001b[39mappend(shapely\u001b[38;5;241m.\u001b[39mgeometry\u001b[38;5;241m.\u001b[39mLineString(segcoords))\n",
      "\u001b[1;31mCutError\u001b[0m: Dual/multiple intersection in a single seg... ugh!  Intersection is of type '<class 'shapely.geometry.linestring.LineString'>'"
     ]
    }
   ],
   "source": [
    "include_rivers=True\n",
    "\n",
    "if include_rivers:  \n",
    "    # download/collect the river network within that shape's bounds\n",
    "    _, reaches = watershed_workflow.get_reaches(sources['hydrography'], hint, \n",
    "                                                watershed.exterior(), crs, crs,\n",
    "                                                in_network=True, properties=True)\n",
    "    \n",
    "    rivers = watershed_workflow.construct_rivers(reaches, method='hydroseq',\n",
    "                                                 ignore_small_rivers=ignore_small_rivers,\n",
    "                                                 prune_by_area=prune_by_area_fraction * watershed.exterior().area * 1.e-6,\n",
    "                                                 remove_diversions=True,\n",
    "                                                 remove_braided_divergences=True)\n",
    "    \n",
    "    # keeping the originals\n",
    "    rivers_orig=[river.deepcopy() for river in rivers]\n",
    "    watershed_orig=copy.deepcopy(watershed) \n",
    "    \n",
    "    # simplifying \n",
    "    rivers = watershed_workflow.simplify(watershed, rivers, simplify_hucs=simplify, simplify_rivers=simplify,\n",
    "                               snap_tol=0.75*simplify, cut_intersections=True)\n",
    "    \n",
    "    # for plotting purpose only\n",
    "    rivers_simplified=[river.deepcopy() for river in rivers] \n",
    "    watershed_simplified=copy.deepcopy(watershed) \n",
    "\n",
    "else:\n",
    "    reaches = []\n",
    "    rivers = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original watershed and river trees are kept for resampling at a desired resolution in the final watershed and rivertree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting original and simplified-pruned rivers and watershed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [20, 20]\n",
    "\n",
    "fig, axs = plt.subplots(1,2,subplot_kw={'projection':watershed_workflow.crs.to_cartopy(crs)})\n",
    "\n",
    "axs[0].plot(watershed_orig.exterior().exterior.xy[0], watershed_orig.exterior().exterior.xy[1], 'k-x')\n",
    "axs[0].set_title('original river network and hucs',fontsize=16)\n",
    "axs[1].plot(watershed.exterior().exterior.xy[0], watershed.exterior().exterior.xy[1], 'k-x')\n",
    "axs[1].set_title('after simplify and prune',fontsize=16)\n",
    "\n",
    "for river in rivers_orig:\n",
    "    for node in river.preOrder():\n",
    "        x,y=node.segment.xy \n",
    "        axs[0].plot(x,y,'-o',markersize=5)\n",
    "\n",
    "for river in rivers_simplified:\n",
    "    for node in river.preOrder():\n",
    "        x,y=node.segment.xy \n",
    "        axs[1].plot(x,y,'-o',markersize=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Densification of River Network and Watershed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d0 = refine_d0; d1 = refine_d1\n",
    "L0 = refine_L0; L1 = refine_L1 \n",
    "\n",
    "# densify_watershed\n",
    "watershed_workflow.densification.densify_hucs(watershed, watershed_orig, rivers, limit_scales=[d0,L0,d1,L1]) \n",
    "\n",
    "#densify_river\n",
    "watershed_workflow.densification.densify_rivers(rivers, rivers_orig, limit=70)\n",
    "\n",
    "\n",
    "\n",
    "# treat sharp angles\n",
    "watershed_workflow.densification.remove_sharp_angles(rivers, watershed, angle_limit=10, junction_angle_limit=10, huc_seg_river_angle_limit=10, limit=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing original, simplified and re-densified huc and river\n",
    "plt.rcParams['figure.figsize'] = [30, 20]\n",
    "\n",
    "fig, axs = plt.subplots(1,3,subplot_kw={'projection':watershed_workflow.crs.to_cartopy(crs)})\n",
    "\n",
    "axs[0].plot(watershed_orig.exterior().exterior.xy[0], watershed_orig.exterior().exterior.xy[1], 'k-x')\n",
    "axs[0].set_title('original river network and hucs',fontsize=16)\n",
    "axs[1].plot(watershed_simplified.exterior().exterior.xy[0], watershed_simplified.exterior().exterior.xy[1], 'k-x')\n",
    "axs[1].set_title('after simplify and prune',fontsize=16)\n",
    "axs[2].plot(watershed.exterior().exterior.xy[0], watershed.exterior().exterior.xy[1], 'k-x')\n",
    "axs[2].set_title('re-densified',fontsize=16)\n",
    "axs[2].plot()\n",
    "\n",
    "for river in rivers_orig:\n",
    "    for node in river.preOrder():\n",
    "        x,y=node.segment.xy \n",
    "        axs[0].plot(x,y,'-o',markersize=5)\n",
    "\n",
    "for river in rivers_simplified:\n",
    "    for node in river.preOrder():\n",
    "        x,y=node.segment.xy \n",
    "        axs[1].plot(x,y,'-o',markersize=5)\n",
    "\n",
    "for river in rivers:\n",
    "    for node in river.preOrder():\n",
    "        x,y=node.segment.xy \n",
    "        axs[2].plot(x,y,'-o',markersize=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many types of Stream Orders are there\n",
    "set([r.properties[\"StreamOrder\"] for r in rivers[0].preOrder()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meshing Begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Triangulation\n",
    "\n",
    "# Refine triangles if they get too acute\n",
    "min_angle = 32 # degrees\n",
    "\n",
    "# width of reach by stream order (order:width)\n",
    "widths = dict({1:8,2:12,3:16})\n",
    "\n",
    "mesh_points2, conn_list, areas, dists = watershed_workflow.tessalate_river_aligned(watershed,rivers, river_width=widths,\n",
    "                                              refine_min_angle=min_angle,refine_max_area=30000,\n",
    "                                              diagnostics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a raster for the elevation map, based on NED\n",
    "dem_profile, dem = watershed_workflow.get_raster_on_shape(sources['DEM'], watershed.exterior(), crs)\n",
    "\n",
    "# elevate the triangle nodes to the dem\n",
    "mesh_points3 = watershed_workflow.elevate(mesh_points2, crs, dem, dem_profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the 2D mesh\n",
    "m2 = watershed_workflow.mesh.Mesh2D(mesh_points3.copy(), conn_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hydrologically condition the mesh, removing pits\n",
    "river_mask=np.zeros((len(m2.conn)))\n",
    "for i, elem in enumerate(m2.conn):\n",
    "    if not len(elem)==3:\n",
    "        river_mask[i]=1     \n",
    "watershed_workflow.condition.fill_pits_dual(m2, is_waterbody=river_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conditioning river mesh\n",
    "#\n",
    "# adding elevations to the river tree for stream bed conditioning\n",
    "watershed_workflow.condition.elevate_rivers(rivers, crs, dem, dem_profile)\n",
    "\n",
    "# conditioning the river mesh using NHD elevations\n",
    "watershed_workflow.condition.condition_river_mesh(m2, river)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting surface mesh with elevations\n",
    "start=min(m2.centroids[:,2])\n",
    "step=(max(m2.centroids[:,2])-(min(m2.centroids[:,2])))/40\n",
    "stop=max(m2.centroids[:,2])+step\n",
    "legend_values=np.arange(start,stop,step)\n",
    "indices, cmap, norm, ticks, labels = watershed_workflow.colors.generate_indexed_colormap(legend_values, cmap='jet')\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = watershed_workflow.plot.get_ax(crs, fig, window=[0.05,0.1,0.9,0.8])\n",
    "ax2 = ax.inset_axes([0.65,0.05,0.3,0.5])\n",
    "cbax = fig.add_axes([0.05,0.05,0.9,0.05])\n",
    "\n",
    "mp = watershed_workflow.plot.mesh(m2, crs, ax=ax, \n",
    "                        linewidth=0.5 ,color=m2.centroids[:,2], \n",
    "                        cmap=cmap, norm = norm, edgecolor='k', facecolor='color')\n",
    "cbar = fig.colorbar(mp, orientation=\"horizontal\", cax=cbax)\n",
    "ax.set_title('surface mesh with elevations')\n",
    "ax.set_aspect('equal', 'datalim')\n",
    "\n",
    "mp2 = watershed_workflow.plot.mesh(m2, crs, ax=ax2, \n",
    "                        linewidth=0.5 ,color=m2.centroids[:,2], \n",
    "                        cmap=cmap, norm = norm, edgecolor='k', facecolor='color')\n",
    "ax2.set_aspect('equal', 'datalim')\n",
    "\n",
    "xlim = (1.4433e6, 1.4438e6)\n",
    "ylim = (-647000, -647500)\n",
    "\n",
    "ax2.set_xlim(xlim)\n",
    "ax2.set_ylim(ylim)\n",
    "ax2.set_xticks([])\n",
    "ax2.set_yticks([])\n",
    "\n",
    "ax.indicate_inset_zoom(ax2, edgecolor='k')\n",
    "\n",
    "cbar.ax.set_title('elevation [m]')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Surface properties\n",
    "\n",
    "Meshes interact with data to provide forcing, parameters, and more in the actual simulation.  Specifically, we need vegetation type on the surface to provide information about transpiration and subsurface structure to provide information about water retention curves, etc.\n",
    "\n",
    "We'll start by downloading and collecting land cover from the NLCD dataset, and generate sets for each land cover type that cover the surface.  Likely these will be some combination of grass, deciduous forest, coniferous forest, and mixed forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the NLCD raster\n",
    "lc_profile, lc_raster = watershed_workflow.get_raster_on_shape(sources['land cover'], \n",
    "                                                     watershed.exterior(), crs)\n",
    "\n",
    "# resample the raster to the triangles\n",
    "lc = watershed_workflow.values_from_raster(m2.centroids, crs, lc_raster, lc_profile)\n",
    "\n",
    "# what land cover types did we get?\n",
    "logging.info('Found land cover dtypes: {}'.format(lc.dtype))\n",
    "logging.info('Found land cover types: {}'.format(set(lc)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the NLCD data\n",
    "\n",
    "# -- get the NLCD colormap which uses official NLCD colors and labels\n",
    "nlcd_indices, nlcd_cmap, nlcd_norm, nlcd_ticks, nlcd_labels = \\\n",
    "                watershed_workflow.colors.generate_nlcd_colormap(lc)\n",
    "\n",
    "# plot the image\n",
    "fig = plt.figure(figsize=figsize)\n",
    "ax = watershed_workflow.plot.get_ax(crs, fig)\n",
    "\n",
    "polys = watershed_workflow.plot.mesh(m2, crs, ax=ax, color=lc, cmap=nlcd_cmap, \n",
    "                                     norm=nlcd_norm, edgecolor='none', \n",
    "                                     facecolor='color', linewidth=0.5)\n",
    "watershed_workflow.colors.colorbar_index(ncolors=len(nlcd_indices), cmap=nlcd_cmap, \n",
    "                                         labels=nlcd_labels) \n",
    "ax.set_title(\"NLCD land cover index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add regions for the land cover types, river corridor and reaches of each order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add labeled sets to the mesh for NLCD\n",
    "nlcd_labels_dict = dict(zip(nlcd_indices, nlcd_labels))\n",
    "watershed_workflow.regions.add_nlcd_labeled_sets(m2, lc, nlcd_labels_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add labeled sets for river corridor cells \n",
    "watershed_workflow.regions.add_river_corridor_regions(m2, rivers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add labeled sets for river corridor cells for reaches each stream order \n",
    "watershed_workflow.regions.add_regions_by_stream_order_rivers(m2, rivers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## example of adding region for a reach by reach id\n",
    "watershed_workflow.regions.add_region_by_reach_id(m2, rivers, reach_ids=[str(25000400040729)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ls in m2.labeled_sets:\n",
    "    print(f'{ls.setid} : {ls.entity} : {len(ls.ent_ids)} : \"{ls.name}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsurface properties\n",
    "\n",
    "The default model uses GLHYMPS to identify geologic formations, and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the NRCS soils data as shapes and project it onto the mesh\n",
    "#\n",
    "soil_profile, soil_survey, soil_survey_props = \\\n",
    "        watershed_workflow.get_shapes(sources['soil structure'], list(watershed.polygons()), crs, \n",
    "                                                     crs, properties=True)\n",
    "\n",
    "# -- determine the NRCS mukey for each soil unit; this uniquely identifies soil \n",
    "#    properties\n",
    "soil_ids = list(soil_survey_props['mukey'][:])\n",
    "soil_survey_props.set_index('mukey', inplace=True)\n",
    "\n",
    "# -- color a raster by the polygons (this makes identifying a triangle's value much \n",
    "#    more efficient)\n",
    "soil_color_profile, soil_color_raster = watershed_workflow.color_raster_from_shapes(soil_survey, crs, soil_ids,\n",
    "                                                                                    watershed.exterior().bounds, 10, crs, -1)\n",
    "\n",
    "# -- resample the raster to the triangles\n",
    "soil_color = watershed_workflow.values_from_raster(m2.centroids, crs, \n",
    "                                         soil_color_raster, soil_color_profile)\n",
    "soil_survey_props"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select only the soils within the watershed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the soil mukey\n",
    "indices, cmap, norm, ticks, labels = watershed_workflow.colors.generate_indexed_colormap(soil_color, cmap='tab20c')\n",
    "fig, ax = watershed_workflow.plot.get_ax(crs)\n",
    "\n",
    "mp = watershed_workflow.plot.mesh(m2, crs, ax=ax, facecolor='color',\n",
    "                        linewidth=0, color=soil_color, \n",
    "                        cmap=cmap, norm = norm\n",
    "                       )\n",
    "\n",
    "watershed_workflow.colors.colorbar_index(ncolors=len(np.unique(soil_color)), cmap=cmap, labels=labels) \n",
    "\n",
    "ax.set_title('soil type index')\n",
    "ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what does soil thickness look like?\n",
    "soil_thickness = np.empty(soil_color.shape, 'd')\n",
    "for mukey in soil_survey_props.index:\n",
    "    soil_thickness[soil_color == mukey] = soil_survey_props.loc[mukey,'thickness [cm]']\n",
    "\n",
    "soil_thickness = soil_thickness / 100\n",
    "indices, cmap, norm, ticks, labels = watershed_workflow.colors.generate_indexed_colormap(soil_thickness, cmap='jet')\n",
    "fig, ax = watershed_workflow.plot.get_ax(crs, figsize=figsize)\n",
    "mp = watershed_workflow.plot.mesh(m2, crs, ax=ax, facecolor='color',\n",
    "                        linewidth=0.5, color=soil_thickness, \n",
    "                        cmap=cmap                       )\n",
    "ax.set_title('soil thickness [m]')\n",
    "cb = fig.colorbar(mp, fraction=0.04, pad=0.04, extend = \"both\", shrink = 0.6)\n",
    "#watershed_workflow.colors.colorbar_index(ncolors=len(np.unique(soil_thickness))//10, cmap=cmap, labels = labels) \n",
    "ax.axis('off')\n",
    "plt.tight_layout()\n",
    "print('Median soil thickness [-] = ', np.nanmedian(soil_thickness))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot of porosity from SSURGO\n",
    "iprop = np.empty(soil_color.shape, 'd')\n",
    "for mukey in soil_survey_props.index:\n",
    "    iprop[soil_color == mukey] = soil_survey_props.loc[ mukey,'porosity [-]']\n",
    "indices, cmap, norm, ticks, labels = watershed_workflow.colors.generate_indexed_colormap(iprop, cmap='jet')\n",
    "fig, ax = watershed_workflow.plot.get_ax(crs, figsize=figsize)\n",
    "mp = watershed_workflow.plot.mesh(m2, crs, ax=ax,\n",
    "                        linewidth=0.5, color=iprop, cmap=cmap, facecolor='color')\n",
    "ax.set_title('soil porosity [-]')\n",
    "cb = fig.colorbar(mp, fraction=0.04, pad=0.04, extend = \"both\", shrink = 0.6)\n",
    "ax.axis('off')\n",
    "plt.tight_layout()\n",
    "print('Median porosity [-] = ', np.nanmedian(iprop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot of permeability\n",
    "iprop = np.empty(soil_color.shape, 'd')\n",
    "for mukey in soil_survey_props.index:\n",
    "    iprop[soil_color == mukey] = soil_survey_props.loc[ mukey,'permeability [m^2]']\n",
    "    \n",
    "indices, cmap, norm, ticks, labels = watershed_workflow.colors.generate_indexed_colormap(np.log10(iprop), cmap='jet')\n",
    "fig, ax = watershed_workflow.plot.get_ax(crs)\n",
    "mp = watershed_workflow.plot.mesh(m2, crs, ax=ax,\n",
    "                        linewidth=0,color=np.log10(iprop),cmap=cmap, facecolor='color')\n",
    "ax.set_title('soil permeability [-]')\n",
    "cb = fig.colorbar(mp, fraction=0.04, pad=0.04, extend = \"both\", shrink = 0.6)\n",
    "cb.ax.set_title('log K')\n",
    "ax.axis('off')\n",
    "print('Min k [m^2] = ', np.nanmin(iprop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note the missing data (white).  This is because some SSURGO map units have no formation with complete \n",
    "# information.  So we merge the above available data, filling where possible and dropping regions that\n",
    "# do not have a complete set of properties.\n",
    "soil_survey_props_clean = soil_survey_props.reset_index()\n",
    "\n",
    "# later scripts expect 'native_index' as a standard name of holding onto the original IDs\n",
    "soil_survey_props_clean.rename_axis('native_index', inplace=True)\n",
    "soil_survey_props_clean.rename(columns={'mukey':'native_index'}, inplace=True)\n",
    "\n",
    "# need thickness in m\n",
    "soil_survey_props_clean['thickness [cm]'] = soil_survey_props_clean['thickness [cm]']/100.\n",
    "soil_survey_props_clean.rename(columns={'thickness [cm]':'thickness [m]'}, inplace=True)\n",
    "\n",
    "\n",
    "def replace_column_nans(df, col_nan, col_replacement):\n",
    "    \"\"\"In a df, replace col_nan entries by col_replacement if is nan.  In Place!\"\"\"\n",
    "    row_indexer = df[col_nan].isna()\n",
    "    df.loc[row_indexer, col_nan] = df.loc[row_indexer, col_replacement]\n",
    "    return\n",
    "\n",
    "# where poro or perm is nan, put Rosetta poro\n",
    "replace_column_nans(soil_survey_props_clean, 'porosity [-]', 'Rosetta porosity [-]')\n",
    "replace_column_nans(soil_survey_props_clean, 'permeability [m^2]', 'Rosetta permeability [m^2]')\n",
    "\n",
    "# drop unnecessary columns\n",
    "for col in ['Rosetta porosity [-]', 'Rosetta permeability [m^2]', 'bulk density [g/cm^3]', 'total sand pct [%]',\n",
    "            'total silt pct [%]', 'total clay pct [%]']:\n",
    "    soil_survey_props_clean.pop(col)\n",
    "    \n",
    "# drop nans\n",
    "soil_survey_props_clean.dropna(inplace=True)\n",
    "soil_survey_props_clean.reset_index(drop=True, inplace=True)\n",
    "\n",
    "soil_survey_props_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new soil_color, keeping on those that are kept here and re-indexing to ATS indices\n",
    "soil_color_new = -np.ones_like(soil_color)\n",
    "for new_id, mukey in enumerate(soil_survey_props_clean['native_index']):\n",
    "    soil_color_new[np.where(soil_color == mukey)] = 1000+new_id\n",
    " \n",
    "\n",
    "# image the new soil_color\n",
    "indices, cmap, norm, ticks, labels = watershed_workflow.colors.generate_indexed_colormap(soil_color_new, cmap='tab20c')\n",
    "fig, ax = watershed_workflow.plot.get_ax(crs)\n",
    "\n",
    "mp = watershed_workflow.plot.mesh(m2, crs, ax=ax, facecolor='color',\n",
    "                    linewidth=0, color=soil_color_new, \n",
    "                    cmap=cmap, norm=norm)\n",
    "\n",
    "watershed_workflow.colors.colorbar_index(ncolors=len(np.unique(soil_color_new)), cmap=cmap, labels=labels) \n",
    "\n",
    "ax.set_title('soil type index')\n",
    "ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLYHMPS geologic layer\n",
    "\n",
    "GLYHMPS is complete in that it does not appear to have missing data, but does not have texture properties needed for Water Retention Models.  Instead we rely on scaling laws to fill the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the GLYHMPS geologic structure data as shapes and project it onto the mesh\n",
    "target_bounds = watershed.exterior().bounds\n",
    "logging.info('target bounds: {}'.format(target_bounds))\n",
    "\n",
    "_, geo_survey, geo_survey_props = watershed_workflow.get_shapes(sources['geologic structure'], \n",
    "                                                      target_bounds, crs, crs, properties=True)\n",
    "\n",
    "# -- log the bounds targeted and found\n",
    "logging.info('shape union bounds: {}'.format(\n",
    "    shapely.ops.cascaded_union(geo_survey).bounds))\n",
    "\n",
    "# -- determine the ID for each soil unit; this uniquely identifies formation\n",
    "#    properties\n",
    "geo_ids = np.array([shp.properties['id'] for shp in geo_survey], np.int32)\n",
    "\n",
    "# -- color a raster by the polygons (this makes identifying a triangle's value much \n",
    "#    more efficient)\n",
    "geo_color_profile, geo_color_raster = \\\n",
    "            watershed_workflow.color_raster_from_shapes(geo_survey, crs, geo_ids,\n",
    "                                                        target_bounds, 10, crs, -1)\n",
    "\n",
    "# -- resample the raster to the triangles\n",
    "geo_color = watershed_workflow.values_from_raster(m2.centroids, crs, \n",
    "                                         geo_color_raster, geo_color_profile)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the properties that appear in the mesh\n",
    "geo_survey_props.set_index('id', inplace=True, drop=False)\n",
    "geo_survey_props = geo_survey_props.loc[np.unique(geo_color), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the geologic formation id\n",
    "indices, cmap, norm, ticks, labels = watershed_workflow.colors.generate_indexed_colormap(geo_color, cmap='tab20b')\n",
    "\n",
    "fig, ax = watershed_workflow.plot.get_ax(crs)\n",
    "\n",
    "mp = watershed_workflow.plot.mesh(m2, crs, ax=ax, facecolor='color',\n",
    "                    linewidth=0, color=geo_color, \n",
    "                    cmap=cmap, norm=norm)\n",
    "\n",
    "watershed_workflow.colors.colorbar_index(ncolors=len(np.unique(geo_color)), cmap=cmap, labels=labels) \n",
    "\n",
    "ax.set_title('geol type index')\n",
    "ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot permeability of the underlying geologic layer\n",
    "iprop = np.empty(geo_color.shape, 'd')\n",
    "for i in geo_survey_props.index:\n",
    "    iprop[geo_color == i] = geo_survey_props.loc[i, 'permeability [m^2]']\n",
    "    \n",
    "indices, cmap, norm, ticks, labels = watershed_workflow.colors.generate_indexed_colormap(np.log10(iprop), cmap='jet')\n",
    "fig, ax = watershed_workflow.plot.get_ax(crs)\n",
    "mp = watershed_workflow.plot.mesh(m2, crs, ax=ax,\n",
    "                        linewidth=0.5,color=np.log10(iprop),cmap=cmap, facecolor='color')\n",
    "cbar = fig.colorbar(mp, shrink=0.5)\n",
    "ax.set_title('geology log permeability [m^2]')\n",
    "ax.axis('off')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot porosity of the geologic layer\n",
    "iprop = np.empty(geo_color.shape, 'd')\n",
    "for i in geo_survey_props.index:\n",
    "    iprop[geo_color == i] = geo_survey_props.loc[i, 'porosity [-]']\n",
    "\n",
    "indices, cmap, norm, ticks, labels = watershed_workflow.colors.generate_indexed_colormap(iprop, cmap='jet')\n",
    "fig, ax = watershed_workflow.plot.get_ax(crs)\n",
    "mp = watershed_workflow.plot.mesh(m2, crs, ax=ax,\n",
    "                        linewidth=0.5,color=np.log10(iprop),cmap=cmap, facecolor='color')\n",
    "cbar = fig.colorbar(mp, shrink=0.5)\n",
    "ax.set_title('geology porosity [-]')\n",
    "ax.axis('off')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note there are clearly some common regions -- no need to duplicate those with identical values.\n",
    "geo_survey_props_clean = geo_survey_props.copy()\n",
    "geo_survey_props_clean.pop('logk_stdev [-]')\n",
    "geo_survey_props_clean.rename(columns={'id':'native_index'}, inplace=True)\n",
    "\n",
    "\n",
    "def reindex_remove_duplicates(df, index=None):\n",
    "    \"\"\"Removes duplicates, creating a new index and saving the old index as tuples of duplicate values. In place!\"\"\"\n",
    "    if index is not None:\n",
    "        if index in df:\n",
    "            df.set_index(index, drop=True, inplace=True)\n",
    "    \n",
    "    index_name = df.index.name\n",
    "\n",
    "    # identify duplicate rows\n",
    "    duplicates = list(df.groupby(list(df)).apply(lambda x: tuple(x.index)))\n",
    "\n",
    "    # order is preserved\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    df.reset_index(inplace=True)\n",
    "    df[index_name] = duplicates\n",
    "    return\n",
    "\n",
    "reindex_remove_duplicates(geo_survey_props_clean, 'native_index')\n",
    "geo_survey_props_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new geologic layer color, keeping on those that are kept here and re-indexing to ATS indices\n",
    "geo_color_new = -np.ones_like(geo_color)\n",
    "for new_id, old_id_dups in enumerate(geo_survey_props_clean['native_index']):\n",
    "    for old_id in old_id_dups:\n",
    "        geo_color_new[np.where(geo_color == old_id)] = 100+new_id\n",
    "    \n",
    "# image the new geo_color\n",
    "indices, cmap, norm, ticks, labels = watershed_workflow.colors.generate_indexed_colormap(geo_color_new, cmap='tab20c')\n",
    "fig, ax = watershed_workflow.plot.get_ax(crs)\n",
    "\n",
    "mp = watershed_workflow.plot.mesh(m2, crs, ax=ax, facecolor='color',\n",
    "                    linewidth=0, color=geo_color_new, \n",
    "                    cmap=cmap, norm=norm)\n",
    "\n",
    "watershed_workflow.colors.colorbar_index(ncolors=len(np.unique(geo_color_new)), cmap=cmap, labels=labels) \n",
    "\n",
    "ax.set_title('geologic type index')\n",
    "ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depth-to-bedrock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depth to bedrock is taken from the [SoilGrids](http://globalchange.bnu.edu.cn/research/dtb.jsp) product.  Here we download a US-based, clipped version of this global product, as file sizes are quite large (all products potentially used total over 100GB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTB_profile, DTB_raster = watershed_workflow.get_raster_on_shape(sources['depth to bedrock'], watershed.exterior(), crs, \n",
    "                                                       nodata=-99999)\n",
    "\n",
    "# resample the raster to the triangles\n",
    "DTB_raster = DTB_raster/100 #convert from cm to m\n",
    "DTB = watershed_workflow.values_from_raster(m2.centroids, crs, DTB_raster, DTB_profile, algorithm='piecewise bilinear')\n",
    "DTB = np.where(DTB >= 0, DTB, np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the resulting surface mesh\n",
    "fig, ax = watershed_workflow.plot.get_ax(crs, window=[0.05,0.1,0.9,0.8])\n",
    "cbax = fig.add_axes([.95,0.1,0.05,0.8])\n",
    "\n",
    "indices, cmap, norm, ticks, labels = watershed_workflow.colors.generate_indexed_colormap(DTB, cmap='plasma_r')\n",
    "mp = watershed_workflow.plot.mesh(m2, crs, ax=ax,\n",
    "                        linewidth=0,color=DTB,cmap=cmap, facecolor='color')\n",
    "cbar = fig.colorbar(mp, orientation=\"vertical\", cax=cbax)\n",
    "\n",
    "ax.set_aspect('equal', 'datalim')\n",
    "ax.axis('off')\n",
    "\n",
    "cbar.ax.set_title('DTB [m]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A combined, complete product?\n",
    "\n",
    "As a default, we would like a material-driven (e.g. not fields for porosity, perm, etc, but soil classes, each with a common porosity/permeability/vG curve) default that is valid everywhere.  That makes it clear that we must rely on GLHYMPS as the only material-based product that is valid everywhere.  Other products may be layered on top of this, replacing GLHYMPS values, but the underlying layer should be based on GLHYMPS.  To fill in the van Genuchten properties, we relate alpha to permeability and choose a single common n and s_r.\n",
    "\n",
    "Where available, we then choose to use SSURGO as a layer on top of GLHYMPS.  So start by using all GLHYMPS values, then override ones where SSURGO is valid with those values.  This will be the second model, and has then three layers -- a bedrock layer, a soil layer from 0 to 2m, and a geologic layer, using GLHYMPS values.  SoilGrids depth-to-bedrock will be used to provide the transition between bedrock and (where > 2m) the GLHYMPS \"geologic\" layer or (where < 2m) the SSURGO \"soil\" layer.  Where SSURGO has no values, the underlying GLHYMPS values will be used even in the top 2m.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "First, all integer IDs in Exodus files must be unique.  This includes Material IDs, side sets, etc.  We create the Material ID map and data frame.  This is used to standardize IDs from multiple data sources.  Traditionally, ATS numbers Material IDs/Side Sets as:\n",
    "\n",
    "* 0-9 : reserved for boundaries, surface/bottom, etc\n",
    "* 10-99 : Land Cover side sets, typically NLCD IDs are used\n",
    "* 100-999 : geologic layer material IDs. 999 is reserved for bedrock.\n",
    "* 1000-9999 : soil layer material IDs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map SSURGO mukey to ATS_ID\n",
    "soil_survey_props_clean['ats_id'] = range(1000, 1000+len(soil_survey_props_clean))\n",
    "soil_survey_props_clean.set_index('ats_id', inplace=True)\n",
    "\n",
    "# map GLHYMPS id to ATS_ID\n",
    "geo_survey_props_clean['ats_id'] = range(100, 100+len(geo_survey_props_clean))\n",
    "geo_survey_props_clean.set_index('ats_id', inplace=True)\n",
    "\n",
    "bedrock_props = watershed_workflow.soil_properties.get_bedrock_properties()\n",
    "\n",
    "# merge the properties databases\n",
    "subsurface_props = pandas.concat([geo_survey_props_clean,\n",
    "                                  soil_survey_props_clean,\n",
    "                                  bedrock_props])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mesh extrusion\n",
    "\n",
    "Given the surface mesh and material IDs on both the surface and subsurface, we can extrude the surface mesh in the vertical to make a 3D mesh.\n",
    "\n",
    "Next we extrude the DEM to create a 3D mesh.\n",
    "\n",
    "The most difficult aspect of extrusion is creating meshes that:\n",
    "1. aren't huge numbers of cells\n",
    "2. aren't huge cell thicknesses, especially near the surface\n",
    "3. follow implied interfaces, e.g. bottom of soil and bottom of geologic layer\n",
    "\n",
    "This is an iterative process that requires some care and some art."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we choose the bottom of the domain to be the maximum of the depth to bedrock.  \n",
    "# This is really up to the user, but we are hard-coding this for this watershed_workflow.\n",
    "dtb_max = np.nanmax(DTB)\n",
    "DTB = np.where(np.isnan(DTB), dtb_max, DTB)\n",
    "\n",
    "total_thickness = np.ceil(DTB.max())\n",
    "print(f'total thickness: {total_thickness} m')\n",
    "\n",
    "total_thickness = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a dz structure for the top 2m of soil\n",
    "# here we try for 10 cells, starting at 5cm at the top and going to 50cm at the bottom of the 2m thick soil\n",
    "dzs, res = watershed_workflow.mesh.optimize_dzs(0.05, 0.5, 2, 10)\n",
    "print(dzs)\n",
    "print(sum(dzs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this looks like it would work out, with rounder numbers:\n",
    "dzs_soil = [0.05, 0.05, 0.05, 0.12, 0.23, 0.5, 0.5, 0.5]\n",
    "print(sum(dzs_soil))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30 m total thickness, minus 2m soil thickness, leaves us with 28 meters to make up.\n",
    "# optimize again...\n",
    "dzs2, res2 = watershed_workflow.mesh.optimize_dzs(1, 10, 28, 6)\n",
    "print(dzs2)\n",
    "print(sum(dzs2))\n",
    "\n",
    "# how about...\n",
    "dzs_geo = [1, 2,3] + 2*[6,]\n",
    "print(dzs_geo)\n",
    "print(sum(dzs_geo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soil structure\n",
    "use_geologic_layer = True\n",
    "\n",
    "# layer extrusion\n",
    "# -- data structures needed for extrusion\n",
    "layer_types = []\n",
    "layer_data = []\n",
    "layer_ncells = []\n",
    "layer_mat_ids = []\n",
    "\n",
    "# -- soil layer --\n",
    "depth = 0\n",
    "for dz in dzs_soil:\n",
    "    depth += 0.5 * dz\n",
    "    layer_types.append('constant')\n",
    "    layer_data.append(dz)\n",
    "    layer_ncells.append(1)\n",
    "    \n",
    "    if use_geologic_layer:\n",
    "        # use glhymps params\n",
    "        br_or_geo = np.where(depth < DTB, geo_color_new, 999)\n",
    "        soil_or_br_or_geo = np.where(np.bitwise_and(soil_color_new > 0, depth < soil_thickness),\n",
    "                                 soil_color_new,\n",
    "                                 br_or_geo)\n",
    "    else:\n",
    "        # use ssurgo down to DTB if it exists\n",
    "        soil_or_geo = np.where(soil_color_new > 0, soil_color_new, geo_color_new)\n",
    "        soil_or_br_or_geo = np.where(depth < DTB, soil_or_geo, 999)\n",
    "    layer_mat_ids.append(soil_or_br_or_geo)\n",
    "    depth += 0.5 * dz\n",
    "    \n",
    "# -- geologic layer --\n",
    "for dz in dzs_geo:\n",
    "    depth += 0.5 * dz\n",
    "    layer_types.append('constant')\n",
    "    layer_data.append(dz)\n",
    "    layer_ncells.append(1)\n",
    "    \n",
    "    if use_geologic_layer:\n",
    "        geo_or_br = np.where(depth < DTB, geo_color_new, 999)\n",
    "    else:\n",
    "        # only soil, no geo\n",
    "        soil_or_geo = np.where(soil_color_new > 0, soil_color_new, geo_color_new)\n",
    "        geo_or_br = np.where(depth < DTB, soil_or_geo, 999)\n",
    "        \n",
    "    layer_mat_ids.append(geo_or_br)\n",
    "    depth += 0.5 * dz\n",
    "\n",
    "# print the summary\n",
    "watershed_workflow.mesh.Mesh3D.summarize_extrusion(layer_types, layer_data, \n",
    "                                            layer_ncells, layer_mat_ids)\n",
    "\n",
    "# downselect subsurface properties to only those that are used\n",
    "layer_mat_id_used = list(np.unique(np.array(layer_mat_ids)))\n",
    "subsurface_props_used = subsurface_props.loc[layer_mat_id_used]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extrude\n",
    "m3 = watershed_workflow.mesh.Mesh3D.extruded_Mesh2D(m2, layer_types, layer_data, \n",
    "                                             layer_ncells, layer_mat_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to disk\n",
    "filename = os.path.join(name, 'output_data', 'BSF_basin')\n",
    "\n",
    "if watershed_workflow.mesh.exodus is not None:\n",
    "    try:\n",
    "        os.remove(filename+'.exo')\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    m3.write_exodus(filename+'.exo')\n",
    "\n",
    "else:\n",
    "    try:\n",
    "        os.remove(filename+'.vtk')\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    m3.write_vtk(filename+'.vtk')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ENV_ATS",
   "language": "python",
   "name": "env_ats"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
